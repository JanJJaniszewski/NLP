{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT Example Notebook\n",
    "\n",
    "This notebooks shows how to train and use the FinBERT pre-trained language model for financial sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:04.902740Z",
     "start_time": "2020-03-23T15:55:04.876252Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from textblob import TextBlob\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from finbert.finbert import *\n",
    "import finbert.utils as tools\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "project_dir = Path.cwd().parent\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:05.711210Z",
     "start_time": "2020-03-23T15:55:05.693609Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting path variables:\n",
    "1. `lm_path`: the path for the pre-trained language model (If vanilla Bert is used then no need to set this one).\n",
    "2. `cl_path`: the path where the classification model is saved.\n",
    "3. `cl_data_path`: the path of the directory that contains the data files of `train.csv`, `validation.csv`, `test.csv`.\n",
    "---\n",
    "\n",
    "In the initialization of `bertmodel`, we can either use the original pre-trained weights from Google by giving `bm = 'bert-base-uncased`, or our further pre-trained language model by `bm = lm_path`\n",
    "\n",
    "\n",
    "---\n",
    "All of the configurations with the model is controlled with the `config` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:07.405597Z",
     "start_time": "2020-03-23T15:55:07.386378Z"
    }
   },
   "outputs": [],
   "source": [
    "lm_path = 'ProsusAI/finbert'\n",
    "cl_path = 'finbert-sentiment'\n",
    "cl_data_path = project_dir/'..'/'Data'/'throughput'/'A_B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Configuring training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the explanations of the training parameters in the class docsctrings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:12.378583Z",
     "start_time": "2020-03-23T15:55:09.196746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean the cl_path\n",
    "try:\n",
    "    shutil.rmtree(cl_path) \n",
    "except:\n",
    "    pass\n",
    "\n",
    "bertmodel = AutoModelForSequenceClassification.from_pretrained(lm_path,cache_dir=None, num_labels=3)\n",
    "\n",
    "\n",
    "config = Config(   data_dir=cl_data_path,\n",
    "                   bert_model=bertmodel,\n",
    "                   num_train_epochs=4,\n",
    "                   model_dir=cl_path,\n",
    "                   max_seq_length = 48,\n",
    "                   train_batch_size = 32,\n",
    "                   learning_rate = 2e-5,\n",
    "                   output_mode='classification',\n",
    "                   warm_up_proportion=0.2,\n",
    "                   local_rank=-1,\n",
    "                   discriminate=True,\n",
    "                   gradual_unfreeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`finbert` is our main class that encapsulates all the functionality. The list of class labels should be given in the prepare_model method call with label_list parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:16.657078Z",
     "start_time": "2020-03-23T15:55:16.639644Z"
    }
   },
   "outputs": [],
   "source": [
    "finbert = FinBert(config)\n",
    "finbert.base_model = 'bert-base-uncased'\n",
    "finbert.config.discriminate=True\n",
    "finbert.config.gradual_unfreeze=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:17.850734Z",
     "start_time": "2020-03-23T15:55:17.368073Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2022 15:58:44 - INFO - finbert.finbert -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "finbert.prepare_model(label_list=['positive','negative','neutral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:19.395707Z",
     "start_time": "2020-03-23T15:55:19.349642Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the training examples\n",
    "train_data = finbert.get_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:55:25.912424Z",
     "start_time": "2020-03-23T15:55:20.065887Z"
    }
   },
   "outputs": [],
   "source": [
    "model = finbert.create_the_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Fine-tune only a subset of the model\n",
    "The variable `freeze` determines the last layer (out of 12) to be freezed. You can skip this part if you want to fine-tune the whole model.\n",
    "\n",
    "<span style=\"color:red\">Important: </span>\n",
    "Execute this step if you want a shorter training time in the expense of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for fine-tuning a subset of the model.\n",
    "\n",
    "freeze = 6\n",
    "\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for i in range(freeze):\n",
    "    for param in model.bert.encoder.layer[i].parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:58:35.486890Z",
     "start_time": "2020-03-23T15:55:27.293772Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
      "05/16/2022 15:58:46 - INFO - finbert.utils -   *** Example ***\n",
      "05/16/2022 15:58:46 - INFO - finbert.utils -   guid: train-1\n",
      "05/16/2022 15:58:46 - INFO - finbert.utils -   tokens: [CLS] thanks ed revenue for the third quarter increased 1 % to ##in ##que ##ncy forecast continues to suggest flat to lower loss rates in 2018 . this trend allowed us to slightly drop our reserve rate during the quarter while maintaining 12 months of forward coverage [SEP]\n",
      "05/16/2022 15:58:46 - INFO - finbert.utils -   input_ids: 101 4283 3968 6599 2005 1996 2353 4284 3445 1015 1003 2000 2378 4226 9407 19939 4247 2000 6592 4257 2000 2896 3279 6165 1999 2760 1012 2023 9874 3039 2149 2000 3621 4530 2256 3914 3446 2076 1996 4284 2096 8498 2260 2706 1997 2830 6325 102\n",
      "05/16/2022 15:58:46 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/16/2022 15:58:46 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/16/2022 15:58:46 - INFO - finbert.utils -   label: positive (id = 0)\n",
      "05/16/2022 15:58:47 - INFO - finbert.finbert -   ***** Loading data *****\n",
      "05/16/2022 15:58:47 - INFO - finbert.finbert -     Num examples = 163\n",
      "05/16/2022 15:58:47 - INFO - finbert.finbert -     Batch size = 32\n",
      "05/16/2022 15:58:47 - INFO - finbert.finbert -     Num steps = 4\n",
      "Epoch:   0%|                                                                                     | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a122c606cd13411ebcdffb30bc3deae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2022 15:59:00 - INFO - finbert.utils -   *** Example ***\n",
      "05/16/2022 15:59:00 - INFO - finbert.utils -   guid: validation-1\n",
      "05/16/2022 15:59:00 - INFO - finbert.utils -   tokens: [CLS] thank you hello , everyone , and welcome to eco ##lab , and market environment , we expect to deliver strong adjusted dil ##uted eps growth in 2017 . and now , here ' s doug baker with some comments thanks that concludes our formal remarks [SEP]\n",
      "05/16/2022 15:59:00 - INFO - finbert.utils -   input_ids: 101 4067 2017 7592 1010 3071 1010 1998 6160 2000 17338 20470 1010 1998 3006 4044 1010 2057 5987 2000 8116 2844 10426 29454 12926 20383 3930 1999 2418 1012 1998 2085 1010 2182 1005 1055 8788 6243 2007 2070 7928 4283 2008 14730 2256 5337 12629 102\n",
      "05/16/2022 15:59:00 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/16/2022 15:59:00 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/16/2022 15:59:00 - INFO - finbert.utils -   label: neutral (id = 2)\n",
      "05/16/2022 15:59:00 - INFO - finbert.finbert -   ***** Loading data *****\n",
      "05/16/2022 15:59:00 - INFO - finbert.finbert -     Num examples = 54\n",
      "05/16/2022 15:59:00 - INFO - finbert.finbert -     Batch size = 32\n",
      "05/16/2022 15:59:00 - INFO - finbert.finbert -     Num steps = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7e52d966c3426f99488716ec65e6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation losses: [2.056371569633484]\n",
      "No best model found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|███████████████████▎                                                         | 1/4 [00:16<00:50, 16.78s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d33f707791847f7bb8d0c6fa2f0d8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2022 15:59:20 - INFO - finbert.utils -   *** Example ***\n",
      "05/16/2022 15:59:20 - INFO - finbert.utils -   guid: validation-1\n",
      "05/16/2022 15:59:20 - INFO - finbert.utils -   tokens: [CLS] thank you hello , everyone , and welcome to eco ##lab , and market environment , we expect to deliver strong adjusted dil ##uted eps growth in 2017 . and now , here ' s doug baker with some comments thanks that concludes our formal remarks [SEP]\n",
      "05/16/2022 15:59:20 - INFO - finbert.utils -   input_ids: 101 4067 2017 7592 1010 3071 1010 1998 6160 2000 17338 20470 1010 1998 3006 4044 1010 2057 5987 2000 8116 2844 10426 29454 12926 20383 3930 1999 2418 1012 1998 2085 1010 2182 1005 1055 8788 6243 2007 2070 7928 4283 2008 14730 2256 5337 12629 102\n",
      "05/16/2022 15:59:20 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/16/2022 15:59:20 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/16/2022 15:59:20 - INFO - finbert.utils -   label: neutral (id = 2)\n",
      "05/16/2022 15:59:21 - INFO - finbert.finbert -   ***** Loading data *****\n",
      "05/16/2022 15:59:21 - INFO - finbert.finbert -     Num examples = 54\n",
      "05/16/2022 15:59:21 - INFO - finbert.finbert -     Batch size = 32\n",
      "05/16/2022 15:59:21 - INFO - finbert.finbert -     Num steps = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90e7d9d87c240fe96a7c9da4c4c136e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation losses: [2.056371569633484, 1.1334471106529236]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|██████████████████████████████████████▌                                      | 2/4 [00:37<00:37, 18.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ae427610984519aa28c2cda04da149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2022 15:59:43 - INFO - finbert.utils -   *** Example ***\n",
      "05/16/2022 15:59:43 - INFO - finbert.utils -   guid: validation-1\n",
      "05/16/2022 15:59:43 - INFO - finbert.utils -   tokens: [CLS] thank you hello , everyone , and welcome to eco ##lab , and market environment , we expect to deliver strong adjusted dil ##uted eps growth in 2017 . and now , here ' s doug baker with some comments thanks that concludes our formal remarks [SEP]\n",
      "05/16/2022 15:59:43 - INFO - finbert.utils -   input_ids: 101 4067 2017 7592 1010 3071 1010 1998 6160 2000 17338 20470 1010 1998 3006 4044 1010 2057 5987 2000 8116 2844 10426 29454 12926 20383 3930 1999 2418 1012 1998 2085 1010 2182 1005 1055 8788 6243 2007 2070 7928 4283 2008 14730 2256 5337 12629 102\n",
      "05/16/2022 15:59:43 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/16/2022 15:59:43 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/16/2022 15:59:43 - INFO - finbert.utils -   label: neutral (id = 2)\n",
      "05/16/2022 15:59:43 - INFO - finbert.finbert -   ***** Loading data *****\n",
      "05/16/2022 15:59:43 - INFO - finbert.finbert -     Num examples = 54\n",
      "05/16/2022 15:59:43 - INFO - finbert.finbert -     Batch size = 32\n",
      "05/16/2022 15:59:43 - INFO - finbert.finbert -     Num steps = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43fcdf74edd4159991f99633873cceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation losses: [2.056371569633484, 1.1334471106529236, 1.1196841597557068]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|█████████████████████████████████████████████████████████▊                   | 3/4 [00:59<00:20, 20.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b28fdf00c0410b848cf615d2c807cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2022 16:00:09 - INFO - finbert.utils -   *** Example ***\n",
      "05/16/2022 16:00:09 - INFO - finbert.utils -   guid: validation-1\n",
      "05/16/2022 16:00:09 - INFO - finbert.utils -   tokens: [CLS] thank you hello , everyone , and welcome to eco ##lab , and market environment , we expect to deliver strong adjusted dil ##uted eps growth in 2017 . and now , here ' s doug baker with some comments thanks that concludes our formal remarks [SEP]\n",
      "05/16/2022 16:00:09 - INFO - finbert.utils -   input_ids: 101 4067 2017 7592 1010 3071 1010 1998 6160 2000 17338 20470 1010 1998 3006 4044 1010 2057 5987 2000 8116 2844 10426 29454 12926 20383 3930 1999 2418 1012 1998 2085 1010 2182 1005 1055 8788 6243 2007 2070 7928 4283 2008 14730 2256 5337 12629 102\n",
      "05/16/2022 16:00:09 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/16/2022 16:00:09 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/16/2022 16:00:09 - INFO - finbert.utils -   label: neutral (id = 2)\n",
      "05/16/2022 16:00:10 - INFO - finbert.finbert -   ***** Loading data *****\n",
      "05/16/2022 16:00:10 - INFO - finbert.finbert -     Num examples = 54\n",
      "05/16/2022 16:00:10 - INFO - finbert.finbert -     Batch size = 32\n",
      "05/16/2022 16:00:10 - INFO - finbert.finbert -     Num steps = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f563c37fb744a1ade1e9ce84fe25a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████████| 4/4 [01:25<00:00, 21.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation losses: [2.056371569633484, 1.1334471106529236, 1.1196841597557068, 1.1258408427238464]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model = finbert.train(train_examples = train_data, model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "`bert.evaluate` outputs the DataFrame, where true labels and logit values for each example is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:58:40.056789Z",
     "start_time": "2020-03-23T15:58:40.023198Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = finbert.get_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:58:48.248044Z",
     "start_time": "2020-03-23T15:58:41.699009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2022 16:00:14 - INFO - finbert.utils -   *** Example ***\n",
      "05/16/2022 16:00:14 - INFO - finbert.utils -   guid: test-1\n",
      "05/16/2022 16:00:14 - INFO - finbert.utils -   tokens: [CLS] thank you , doug , and good morning we ' re to informing the market of our strategy by that time we continue to believe that we will be able to lower our pharmaceutical cost by more than $ 3 billion annually in 2020 and beyond [SEP]\n",
      "05/16/2022 16:00:14 - INFO - finbert.utils -   input_ids: 101 4067 2017 1010 8788 1010 1998 2204 2851 2057 1005 2128 2000 21672 1996 3006 1997 2256 5656 2011 2008 2051 2057 3613 2000 2903 2008 2057 2097 2022 2583 2000 2896 2256 13859 3465 2011 2062 2084 1002 1017 4551 6604 1999 12609 1998 3458 102\n",
      "05/16/2022 16:00:14 - INFO - finbert.utils -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/16/2022 16:00:14 - INFO - finbert.utils -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/16/2022 16:00:14 - INFO - finbert.utils -   label: positive (id = 0)\n",
      "05/16/2022 16:00:14 - INFO - finbert.finbert -   ***** Loading data *****\n",
      "05/16/2022 16:00:14 - INFO - finbert.finbert -     Num examples = 55\n",
      "05/16/2022 16:00:14 - INFO - finbert.finbert -     Batch size = 32\n",
      "05/16/2022 16:00:14 - INFO - finbert.finbert -     Num steps = 4\n",
      "05/16/2022 16:00:14 - INFO - finbert.finbert -   ***** Running evaluation ***** \n",
      "05/16/2022 16:00:14 - INFO - finbert.finbert -     Num examples = 55\n",
      "05/16/2022 16:00:14 - INFO - finbert.finbert -     Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afac9c0ae69c4c1bb60998c1fcec2c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = finbert.evaluate(examples=test_data, model=trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:58:51.361079Z",
     "start_time": "2020-03-23T15:58:51.339548Z"
    }
   },
   "outputs": [],
   "source": [
    "def report(df, cols=['label','prediction','logits']):\n",
    "    #print('Validation loss:{0:.2f}'.format(metrics['best_validation_loss']))\n",
    "    cs = CrossEntropyLoss(weight=finbert.class_weights)\n",
    "    loss = cs(torch.tensor(list(df[cols[2]])),torch.tensor(list(df[cols[0]])))\n",
    "    print(\"Loss:{0:.2f}\".format(loss))\n",
    "    print(\"Accuracy:{0:.2f}\".format((df[cols[0]] == df[cols[1]]).sum() / df.shape[0]) )\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(df[cols[0]], df[cols[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:58:53.190447Z",
     "start_time": "2020-03-23T15:58:53.166729Z"
    }
   },
   "outputs": [],
   "source": [
    "results['prediction'] = results.predictions.apply(lambda x: np.argmax(x,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:58:54.436270Z",
     "start_time": "2020-03-23T15:58:54.399174Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.21\n",
      "Accuracy:0.25\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.24      0.33        25\n",
      "           1       0.23      0.12      0.16        24\n",
      "           2       0.16      0.83      0.27         6\n",
      "\n",
      "    accuracy                           0.25        55\n",
      "   macro avg       0.31      0.40      0.26        55\n",
      "weighted avg       0.37      0.25      0.25        55\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pole1\\.conda\\envs\\finbert\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "report(results,cols=['labels','prediction','predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `predict` function, given a piece of text, we split it into a list of sentences and then predict sentiment for each sentence. The output is written into a dataframe. Predictions are represented in three different columns: \n",
    "\n",
    "1) `logit`: probabilities for each class\n",
    "\n",
    "2) `prediction`: predicted label\n",
    "\n",
    "3) `sentiment_score`: sentiment score calculated as: probability of positive - probability of negative\n",
    "\n",
    "Below we analyze a paragraph taken out of [this](https://www.economist.com/finance-and-economics/2019/01/03/a-profit-warning-from-apple-jolts-markets) article from The Economist. For comparison purposes, we also put the sentiments predicted with TextBlob.\n",
    "> Later that day Apple said it was revising down its earnings expectations in the fourth quarter of 2018, largely because of lower sales and signs of economic weakness in China. The news rapidly infected financial markets. Apple’s share price fell by around 7% in after-hours trading and the decline was extended to more than 10% when the market opened. The dollar fell by 3.7% against the yen in a matter of minutes after the announcement, before rapidly recovering some ground. Asian stockmarkets closed down on January 3rd and European ones opened lower. Yields on government bonds fell as investors fled to the traditional haven in a market storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:59:03.875213Z",
     "start_time": "2020-03-23T15:59:03.857612Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Later that day Apple said it was revising down its earnings expectations in \\\n",
    "the fourth quarter of 2018, largely because of lower sales and signs of economic weakness in China. \\\n",
    "The news rapidly infected financial markets. Apple’s share price fell by around 7% in after-hours \\\n",
    "trading and the decline was extended to more than 10% when the market opened. The dollar fell \\\n",
    "by 3.7% against the yen in a matter of minutes after the announcement, before rapidly recovering \\\n",
    "some ground. Asian stockmarkets closed down on January 3rd and European ones opened lower. \\\n",
    "Yields on government bonds fell as investors fled to the traditional haven in a market storm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:59:16.246963Z",
     "start_time": "2020-03-23T15:59:13.285393Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/C:%5CUsers%5Cpole1%5CPycharmProjects%5CNLP%5CfinBERT-master%5Cmodels%5Cclassifier_model%5Cfinbert-sentiment/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'C:\\Users\\pole1\\PycharmProjects\\NLP\\finBERT-master\\models\\classifier_model\\finbert-sentiment'. Make sure that:\n\n- 'C:\\Users\\pole1\\PycharmProjects\\NLP\\finBERT-master\\models\\classifier_model\\finbert-sentiment' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'C:\\Users\\pole1\\PycharmProjects\\NLP\\finBERT-master\\models\\classifier_model\\finbert-sentiment' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\.conda\\envs\\finbert\\lib\\site-packages\\transformers\\configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    405\u001B[0m                 \u001B[0mlocal_files_only\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlocal_files_only\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 406\u001B[1;33m                 \u001B[0muse_auth_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    407\u001B[0m             )\n",
      "\u001B[1;32m~\\.conda\\envs\\finbert\\lib\\site-packages\\transformers\\file_utils.py\u001B[0m in \u001B[0;36mcached_path\u001B[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[0;32m   1084\u001B[0m             \u001B[0muse_auth_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1085\u001B[1;33m             \u001B[0mlocal_files_only\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlocal_files_only\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1086\u001B[0m         )\n",
      "\u001B[1;32m~\\.conda\\envs\\finbert\\lib\\site-packages\\transformers\\file_utils.py\u001B[0m in \u001B[0;36mget_from_cache\u001B[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001B[0m\n\u001B[0;32m   1214\u001B[0m             \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mallow_redirects\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproxies\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0metag_timeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1215\u001B[1;33m             \u001B[0mr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_for_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1216\u001B[0m             \u001B[0metag\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"X-Linked-Etag\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"ETag\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\finbert\\lib\\site-packages\\requests\\models.py\u001B[0m in \u001B[0;36mraise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    959\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhttp_error_msg\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 960\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhttp_error_msg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    961\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mHTTPError\u001B[0m: 404 Client Error: Not Found for url: https://huggingface.co/C:%5CUsers%5Cpole1%5CPycharmProjects%5CNLP%5CfinBERT-master%5Cmodels%5Cclassifier_model%5Cfinbert-sentiment/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-17-796ffb32ba25>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mcl_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mproject_dir\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;34m'models'\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;34m'classifier_model'\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;34m'finbert-sentiment'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoModelForSequenceClassification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcl_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_labels\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\.conda\\envs\\finbert\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   1275\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1276\u001B[0m             config, kwargs = AutoConfig.from_pretrained(\n\u001B[1;32m-> 1277\u001B[1;33m                 \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturn_unused_kwargs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1278\u001B[0m             )\n\u001B[0;32m   1279\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\finbert\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    347\u001B[0m             \u001B[1;33m{\u001B[0m\u001B[1;34m'foo'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    348\u001B[0m         \"\"\"\n\u001B[1;32m--> 349\u001B[1;33m         \u001B[0mconfig_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_config_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    350\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    351\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m\"model_type\"\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mconfig_dict\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\finbert\\lib\\site-packages\\transformers\\configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    416\u001B[0m                 \u001B[1;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    417\u001B[0m             )\n\u001B[1;32m--> 418\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    419\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    420\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mJSONDecodeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: Can't load config for 'C:\\Users\\pole1\\PycharmProjects\\NLP\\finBERT-master\\models\\classifier_model\\finbert-sentiment'. Make sure that:\n\n- 'C:\\Users\\pole1\\PycharmProjects\\NLP\\finBERT-master\\models\\classifier_model\\finbert-sentiment' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'C:\\Users\\pole1\\PycharmProjects\\NLP\\finBERT-master\\models\\classifier_model\\finbert-sentiment' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "cl_path = project_dir/'models'/'classifier_model'/'finbert-sentiment'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cl_path, cache_dir=None, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:59:19.531663Z",
     "start_time": "2020-03-23T15:59:17.744984Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = predict(text,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:59:20.519047Z",
     "start_time": "2020-03-23T15:59:20.440450Z"
    }
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "result['textblob_prediction'] = [sentence.sentiment.polarity for sentence in blob.sentences]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:59:38.737969Z",
     "start_time": "2020-03-23T15:59:38.718255Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Average sentiment is %.2f.' % (result.sentiment_score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:59:45.922058Z",
     "start_time": "2020-03-23T15:59:45.904622Z"
    }
   },
   "outputs": [],
   "source": [
    "text2 = \"Shares in the spin-off of South African e-commerce group Naspers surged more than 25% \\\n",
    "in the first minutes of their market debut in Amsterdam on Wednesday. Bob van Dijk, CEO of \\\n",
    "Naspers and Prosus Group poses at Amsterdam's stock exchange, as Prosus begins trading on the \\\n",
    "Euronext stock exchange in Amsterdam, Netherlands, September 11, 2019. REUTERS/Piroschka van de Wouw \\\n",
    "Prosus comprises Naspers’ global empire of consumer internet assets, with the jewel in the crown a \\\n",
    "31% stake in Chinese tech titan Tencent. There is 'way more demand than is even available, so that’s \\\n",
    "good,' said the CEO of Euronext Amsterdam, Maurice van Tilburg. 'It’s going to be an interesting \\\n",
    "hour of trade after opening this morning.' Euronext had given an indicative price of 58.70 euros \\\n",
    "per share for Prosus, implying a market value of 95.3 billion euros ($105 billion). The shares \\\n",
    "jumped to 76 euros on opening and were trading at 75 euros at 0719 GMT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:59:48.152474Z",
     "start_time": "2020-03-23T15:59:47.028417Z"
    }
   },
   "outputs": [],
   "source": [
    "result2 = predict(text2,model)\n",
    "blob = TextBlob(text2)\n",
    "result2['textblob_prediction'] = [sentence.sentiment.polarity for sentence in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:59:50.428951Z",
     "start_time": "2020-03-23T15:59:50.402385Z"
    }
   },
   "outputs": [],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T16:00:27.031491Z",
     "start_time": "2020-03-23T16:00:27.012639Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Average sentiment is %.2f.' % (result2.sentiment_score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}